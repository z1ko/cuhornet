/**
 * @author Federico Busato                                                  <br>
 *         Univerity of Verona, Dept. of Computer Science                   <br>
 *         federico.busato@univr.it
 * @date September, 2017
 * @version v2
 *
 * @copyright Copyright Â© 2017 Hornet. All rights reserved.
 *
 * @license{<blockquote>
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * * Redistributions of source code must retain the above copyright notice, this
 *   list of conditions and the following disclaimer.
 * * Redistributions in binary form must reproduce the above copyright notice,
 *   this list of conditions and the following disclaimer in the documentation
 *   and/or other materials provided with the distribution.
 * * Neither the name of the copyright holder nor the names of its
 *   contributors may be used to endorse or promote products derived from
 *   this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 * </blockquote>}
 */
#include "BinarySearchKernel.cuh"
#include "StandardAPI.hpp"
#include <Device/Primitives/CubWrapper.cuh>  //xlib::CubExclusiveSum
#include <Device/Util/DeviceProperties.cuh>  //xlib::SMemPerBlock
#include <thrust/scan.h>                     //thrust::exclusive_scan

namespace hornets_nest {
namespace load_balancing {

template<typename HornetClass>
BinarySearch::BinarySearch(HornetClass& hornet,
                           const float work_factor) noexcept {
    //static_assert(IsHornet<HornetClass>::value,
    //             "BinarySearch: parameter is not an instance of Hornet Class");
    d_work.resize(work_factor * hornet.nV());
}

inline BinarySearch::~BinarySearch() noexcept {
    //hornets_nest::gpu::free(_d_work);
}

template<typename HornetClass, typename Operator, typename vid_t>
void BinarySearch::apply(HornetClass& hornet,
                         const vid_t *      d_input,
                         int                num_vertices,
                         const Operator&    op) const noexcept {

    d_work.resize(num_vertices + 1);
    assert(num_vertices < _work_size && "BinarySearch (work queue) too small");
    //printf("[BSLD] Resized d_work and prefixsum to %d\n", num_vertices + 1);

    if (d_input != nullptr) {
    kernel::computeWorkKernel
        <<< xlib::ceil_div<BLOCK_SIZE>(num_vertices), BLOCK_SIZE >>>
        (hornet.device(), d_input, num_vertices, d_work.data().get());
    } else {
    kernel::computeWorkKernel
        <<< xlib::ceil_div<BLOCK_SIZE>(num_vertices), BLOCK_SIZE >>>
        (hornet.device(), num_vertices, d_work.data().get());
    }
    CHECK_CUDA_ERROR

#if 0
    thrust::host_vector<vid_t> h_input(num_vertices);
    gpu::copyToHost(d_input, num_vertices, h_input.data().get());

    printf("[BSLD] Executed computeWorkKernel, result vector:\n");
    thrust::host_vector<int> h_work = d_work;
    for (unsigned int i = 0; i < h_work.size() - 1; i++)
        printf("[%6d]\tvertex: %6d | work_size: %4d\n", i, h_input[i], h_work[i]);
#endif

    thrust::exclusive_scan(d_work.begin(), d_work.end(), d_work.begin());
    CHECK_CUDA_ERROR

#if 0
    printf("[BSLD] Executed PrefixSum, result vector:\n");
    h_work = d_work;
    for (unsigned int i = 0; i < h_work.size() - 1; i++)
        printf("[%6d]\tvertex: %6d | work_size: %4d\n", i, h_input[i], h_work[i]);
    printf("[BSLD] Total work: %d\n", h_work[h_work.size() - 1]);
#endif

    int total_work;
    cuMemcpyToHost(d_work.data().get() + num_vertices, total_work);
    if (total_work == 0)
        return;

    const unsigned int BLOCK_COUNT = xlib::ceil_div(total_work, BLOCK_SIZE);
    if (d_input != nullptr) {
    kernel::binarySearchKernel<BLOCK_SIZE>
        <<< BLOCK_COUNT, BLOCK_SIZE >>>
        (hornet.device(), d_input, d_work.data().get(), num_vertices + 1, op);
    } else {
    kernel::binarySearchKernel<BLOCK_SIZE>
        <<< BLOCK_COUNT, BLOCK_SIZE >>>
        (hornet.device(), d_work.data().get(), num_vertices + 1, op);
    }
    CHECK_CUDA_ERROR
}

template<typename HornetClass, typename Operator>
void BinarySearch::apply(HornetClass& hornet, const Operator& op)
                         const noexcept {
    apply<HornetClass, Operator, int>(hornet, nullptr, (int) hornet.nV(), op);
}

template<typename HornetClass, typename Operator, template<typename> typename Update, typename vid_t>
void BinarySearch::apply(HornetClass& hornet, Update<vid_t>& batch, const Operator& op, bool reverse) {

    auto soa_ptr = batch.in_edge().get_soa_ptr();
    vid_t *d_input;

    if (reverse)
        d_input = soa_ptr.template get<1>();
    else
        d_input = soa_ptr.template get<0>();

    apply<HornetClass, Operator, vid_t>(hornet, d_input, batch.size(), op);
}

} // namespace load_balancing
} // namespace hornets_nest
